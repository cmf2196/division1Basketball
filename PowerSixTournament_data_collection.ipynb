{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program collects season long data for teams in the Power 5 conferences, and the big east for the years 2014 through 2019.  There are 75 total teams, thus, this scrypt scrapes team data from 375 different webpages , availible on Sports-reference.com.   \n",
    "Additional scraping is done to determine the Win-Loss percentage of each team accross the 6 years of play. This requires a scrape of an additional 75 websites. \n",
    "Finaly, the conference tournament games and their results are scaped. \n",
    "\n",
    "Once the data is collected, it is organized such that for each of the conference tournament games in the past six years, each tournament game, and its result is written into a  pandas dataframe, with the season stats of each team provided as well.\n",
    "\n",
    "All collected data is from Sports-reference.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports we want\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import lxml.html as lh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "####### useful \n",
    "#df.loc[row_selection, col_selection]\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## this will get the games played in tournament play\n",
    "## this will get the standings for each team after postseason play, and provide a list of all big east teams in the conference over time\n",
    "powerSixConf = ['big-east' , 'big-ten' , 'acc' , 'sec' , 'pac-12' , 'big-12'] # power six conference\n",
    "powerSixConfID = [1 ]      # ID's of the power Six Conf\n",
    "for k in range(1 , len(powerSixConf)):\n",
    "    powerSixConfID += [powerSixConfID[k-1] + 1]\n",
    "powerSixTeams = []    # empty list which will be iterated into   \n",
    "powerSixTeamsAndConf = []   # we now also add in the conference ID\n",
    "conferenceTournamentGames = pd.DataFrame()    # empty dataframe\n",
    "\n",
    "\n",
    "for i in range(2015 , 2020 , 1):\n",
    "    for j in range(len(powerSixConf)):\n",
    "    \n",
    "        url = \"https://www.sports-reference.com/cbb/conferences/\" + str(powerSixConf[j]) + \"/\" + str(i) + \"-schedule.html\"   # tournament games\n",
    "        #url2 = \"https://en.wikipedia.org/wiki/\" + str(i) + \"_SEC_Men%27s_Basketball_Tournament\"    # league standings\n",
    "        url2 = \"https://www.sports-reference.com/cbb/conferences/\" + str(powerSixConf[j]) + \"/\" + str(i) + \".html\"       # league standings\n",
    "    #read the data with Beautiful SOup\n",
    "    ############ this is for the tournament games  ##############  \n",
    "    #this can be iterated over a number of conferences as well\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        tables = soup.findAll('table')\n",
    "\n",
    "    # make the dataframe for the tournament data \n",
    "        df = pd.read_html(str(tables))[0]\n",
    "\n",
    "    # we need to clean the data for the tournament \n",
    "        when = df[df['Notes']=='Notes'].index.values.astype(int)\n",
    "        for k in range(len(when)):\t\n",
    "            df.drop(when[k] , inplace=True)\n",
    "        notes = df['Notes']\n",
    "    # this will only get the tournament games\n",
    "        when2 = notes.first_valid_index()\n",
    "        df2 = df.loc[when2:]\n",
    "\n",
    "    ### now lets add on a column of the year\n",
    "        numRows = len(df2.index)\n",
    "        currYear = [i] * numRows\n",
    "        df2['Date'] = currYear   # we only care about the year not the date\n",
    "        conferenceTournamentGames = conferenceTournamentGames.append(df2 , ignore_index=True) \n",
    "\n",
    "\n",
    "    ## this is for the standings \n",
    "        pageSt = requests.get(url2)\n",
    "        soupSt = BeautifulSoup(pageSt.content, \"lxml\")\n",
    "        tablesSt = soupSt.find('table')\n",
    "\n",
    "\n",
    "    # make the dataframe for the standings data\n",
    "        df2 = pd.read_html(str(tablesSt))[0]\n",
    "\n",
    "        standings = df2[df2.columns[[0,1]]]\n",
    "\n",
    "        ## this will generate a list of all the teams that are in the power six conf for the years in question\n",
    "        for p in range(len(standings.index)):\n",
    "            team = standings.iloc[p,1]\n",
    "            team = team.lower()\n",
    "            team = team.replace(' ' , '-')\n",
    "            team = team.replace('(' , '')\n",
    "            team = team.replace(')' , '')\n",
    "            team = team.replace('.' , '')\n",
    "            team = team.replace( \"'\" , '')\n",
    "            team = team.replace(\"&\" , \"\")\n",
    "            team = team.replace(\"university-of-\" , \"\")            \n",
    "            if team not in powerSixTeams:\n",
    "                powerSixTeams += [team]\n",
    "            combo = [team , powerSixConfID[j]]\n",
    "            if (combo not in powerSixTeamsAndConf) and (i == 2018):   #this accounts for teams switching conferences\n",
    "                powerSixTeamsAndConf += [[team , powerSixConfID[j]]]\n",
    "\n",
    "\n",
    "    # we should clean the team names in the conferenceTournamentGames df in the same way\n",
    "        for m in range(len(conferenceTournamentGames.index)):\n",
    "            team1 = conferenceTournamentGames.iloc[m,1]\n",
    "            team1 = team1.lower()\n",
    "            team1 = team1.replace(' ' , '-')\n",
    "            team1 = team1.replace('(' , '')\n",
    "            team1 = team1.replace(')' , '')\n",
    "            team1 = team1.replace('.' , '')\n",
    "            team1 = team1.replace( \"'\" , '')\n",
    "            team1 = team1.replace(\"&\" , \"\") \n",
    "            team1 = team1.replace(\"university-of-\" , \"\")  \n",
    "            conferenceTournamentGames.iloc[m,1] = team1\n",
    "\n",
    "            team2 = conferenceTournamentGames.iloc[m,3]\n",
    "            team2 = team2.lower()\n",
    "            team2 = team2.replace(' ' , '-')\n",
    "            team2 = team2.replace('(' , '')\n",
    "            team2 = team2.replace(')' , '')\n",
    "            team2 = team2.replace('.' , '')\n",
    "            team2 = team2.replace( \"'\" , '')\n",
    "            team2 = team2.replace(\"&\" , \"\")   \n",
    "            team2 = team2.replace(\"university-of-\" , \"\")  \n",
    "            conferenceTournamentGames.iloc[m,3] = team2\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# a loop to get the win loss for the teams\n",
    "WLTeams = pd.DataFrame()      # initialize and empty dataframe\n",
    "for j in range(len(powerSixTeams)): \n",
    "    url3 = \"https://www.sports-reference.com/cbb/schools/\" + str(powerSixTeams[j]) + \"/\"\n",
    "    teamNames = powerSixTeams[j] \n",
    "        # get the data\n",
    "    pageWL = requests.get(url3)\n",
    "    soupWL = BeautifulSoup(pageWL.content, \"lxml\")\n",
    "    tablesWL = soupWL.findAll('table')\n",
    "        \n",
    "        # build a dataframe of the win loss data\n",
    "    win_loss_df = pd.read_html(str(tablesWL))[0]\n",
    "    win_loss_6_df = win_loss_df.head(5)\n",
    "    win_loss_6_df['team'] = teamNames\n",
    "\n",
    "    WLTeams = WLTeams.append(win_loss_6_df , ignore_index=True)    \n",
    "# this will get the W-L%\n",
    "    WL = win_loss_df.iloc[:, [5]]\n",
    "\n",
    "    WL20 = WL.values[1:6]\n",
    "        # this gets rid of the wierd index This should be done in the dataframe ahead of time \n",
    "    index = np.argwhere(WL20=='W-L%')\n",
    "    WL20 = np.delete(WL20, index)\n",
    "    \n",
    "# Add a column of the years to the win loss dataframe\n",
    "years = [2019 , 2018 , 2017 , 2016 , 2015 ] * 75     \n",
    "WLTeams['year'] = years\n",
    "# going to need to use an intersection later to add the win loss record for each team for each year to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all we are curious about here are the win loss for out of conference and in conference\n",
    "WLTeams.drop(WLTeams.columns[[0, 1 , 2, 3 , 4, 6 , 7 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 ]], axis=1 , inplace =True)\n",
    "WLTeams.columns = ['WL' , 'WLC' , 'teams' , 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide Team IDS's to each team\n",
    "powerSixTeamID = [1000]    #  list initiated at 1000 for team ID's            \n",
    "for i in range(1, len(powerSixTeams)):\n",
    "    powerSixTeamID += [powerSixTeamID[i-1] + 1]    # this will give ID's to the teams of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 0\n",
      "2015 1\n",
      "2015 2\n",
      "2015 3\n",
      "2015 4\n",
      "2015 5\n",
      "2015 6\n",
      "2015 7\n",
      "2015 8\n",
      "2015 9\n",
      "2015 10\n",
      "2015 11\n",
      "2015 12\n",
      "2015 13\n",
      "2015 14\n",
      "2015 15\n",
      "2015 16\n",
      "2015 17\n",
      "2015 18\n",
      "2015 19\n",
      "2015 20\n",
      "2015 21\n",
      "2015 22\n",
      "2015 23\n",
      "2015 24\n",
      "2015 25\n",
      "2015 26\n",
      "2015 27\n",
      "2015 28\n",
      "2015 29\n",
      "2015 30\n",
      "2015 31\n",
      "2015 32\n",
      "2015 33\n",
      "2015 34\n",
      "2015 35\n",
      "2015 36\n",
      "2015 37\n",
      "2015 38\n",
      "2015 39\n",
      "2015 40\n",
      "2015 41\n",
      "2015 42\n",
      "2015 43\n",
      "2015 44\n",
      "2015 45\n",
      "2015 46\n",
      "2015 47\n",
      "2015 48\n",
      "2015 49\n",
      "2015 50\n",
      "2015 51\n",
      "2015 52\n",
      "2015 53\n",
      "2015 54\n",
      "2015 55\n",
      "2015 56\n",
      "2015 57\n",
      "2015 58\n",
      "2015 59\n",
      "2015 60\n",
      "2015 61\n",
      "2015 62\n",
      "2015 63\n",
      "2015 64\n",
      "2015 65\n",
      "2015 66\n",
      "2015 67\n",
      "2015 68\n",
      "2015 69\n",
      "2015 70\n",
      "2015 71\n",
      "2015 72\n",
      "2015 73\n",
      "2015 74\n",
      "2016 0\n",
      "2016 1\n",
      "2016 2\n",
      "2016 3\n",
      "2016 4\n",
      "2016 5\n",
      "2016 6\n",
      "2016 7\n",
      "2016 8\n",
      "2016 9\n",
      "2016 10\n",
      "2016 11\n",
      "2016 12\n",
      "2016 13\n",
      "2016 14\n",
      "2016 15\n",
      "2016 16\n",
      "2016 17\n",
      "2016 18\n",
      "2016 19\n",
      "2016 20\n",
      "2016 21\n",
      "2016 22\n",
      "2016 23\n",
      "2016 24\n",
      "2016 25\n",
      "2016 26\n",
      "2016 27\n",
      "2016 28\n",
      "2016 29\n",
      "2016 30\n",
      "2016 31\n",
      "2016 32\n",
      "2016 33\n",
      "2016 34\n",
      "2016 35\n",
      "2016 36\n",
      "2016 37\n",
      "2016 38\n",
      "2016 39\n",
      "2016 40\n",
      "2016 41\n",
      "2016 42\n",
      "2016 43\n",
      "2016 44\n",
      "2016 45\n",
      "2016 46\n",
      "2016 47\n",
      "2016 48\n",
      "2016 49\n",
      "2016 50\n",
      "2016 51\n",
      "2016 52\n",
      "2016 53\n",
      "2016 54\n",
      "2016 55\n",
      "2016 56\n",
      "2016 57\n",
      "2016 58\n",
      "2016 59\n",
      "2016 60\n",
      "2016 61\n",
      "2016 62\n",
      "2016 63\n",
      "2016 64\n",
      "2016 65\n",
      "2016 66\n",
      "2016 67\n",
      "2016 68\n",
      "2016 69\n",
      "2016 70\n",
      "2016 71\n",
      "2016 72\n",
      "2016 73\n",
      "2016 74\n",
      "2017 0\n",
      "2017 1\n",
      "2017 2\n",
      "2017 3\n",
      "2017 4\n",
      "2017 5\n",
      "2017 6\n",
      "2017 7\n",
      "2017 8\n",
      "2017 9\n",
      "2017 10\n",
      "2017 11\n",
      "2017 12\n",
      "2017 13\n",
      "2017 14\n",
      "2017 15\n",
      "2017 16\n",
      "2017 17\n",
      "2017 18\n",
      "2017 19\n",
      "2017 20\n",
      "2017 21\n",
      "2017 22\n",
      "2017 23\n",
      "2017 24\n",
      "2017 25\n",
      "2017 26\n",
      "2017 27\n",
      "2017 28\n",
      "2017 29\n",
      "2017 30\n",
      "2017 31\n",
      "2017 32\n",
      "2017 33\n",
      "2017 34\n",
      "2017 35\n",
      "2017 36\n",
      "2017 37\n",
      "2017 38\n",
      "2017 39\n",
      "2017 40\n",
      "2017 41\n",
      "2017 42\n",
      "2017 43\n",
      "2017 44\n",
      "2017 45\n",
      "2017 46\n",
      "2017 47\n",
      "2017 48\n",
      "2017 49\n",
      "2017 50\n",
      "2017 51\n",
      "2017 52\n",
      "2017 53\n",
      "2017 54\n",
      "2017 55\n",
      "2017 56\n",
      "2017 57\n",
      "2017 58\n",
      "2017 59\n",
      "2017 60\n",
      "2017 61\n",
      "2017 62\n",
      "2017 63\n",
      "2017 64\n",
      "2017 65\n",
      "2017 66\n",
      "2017 67\n",
      "2017 68\n",
      "2017 69\n",
      "2017 70\n",
      "2017 71\n",
      "2017 72\n",
      "2017 73\n",
      "2017 74\n",
      "2018 0\n",
      "2018 1\n",
      "2018 2\n",
      "2018 3\n",
      "2018 4\n",
      "2018 5\n",
      "2018 6\n",
      "2018 7\n",
      "2018 8\n",
      "2018 9\n",
      "2018 10\n",
      "2018 11\n",
      "2018 12\n",
      "2018 13\n",
      "2018 14\n",
      "2018 15\n",
      "2018 16\n",
      "2018 17\n",
      "2018 18\n",
      "2018 19\n",
      "2018 20\n",
      "2018 21\n",
      "2018 22\n",
      "2018 23\n",
      "2018 24\n",
      "2018 25\n",
      "2018 26\n",
      "2018 27\n",
      "2018 28\n",
      "2018 29\n",
      "2018 30\n",
      "2018 31\n",
      "2018 32\n",
      "2018 33\n",
      "2018 34\n",
      "2018 35\n",
      "2018 36\n",
      "2018 37\n",
      "2018 38\n",
      "2018 39\n",
      "2018 40\n",
      "2018 41\n",
      "2018 42\n",
      "2018 43\n"
     ]
    }
   ],
   "source": [
    "# this will collect the season data for every team we selected in the list, for the number of indicated years\n",
    "# these for loops will scrape data from 450 different webpages\n",
    "seasonStats = pd.DataFrame()   # start with an empty dataframe\n",
    "for i in range(2015 , 2020, 1):   # this is the years \n",
    "\tfor j in range(len(powerSixTeams)): # teams\n",
    "\t\turl3 = \"https://www.sports-reference.com/cbb/schools/\" + str(powerSixTeams[j]) + \"/\" + str(i) + \".html\" # season data\n",
    "\t\tpageSe = requests.get(url3)   # scrape\n",
    "\t\tsoupSe = BeautifulSoup(pageSe.content, \"lxml\")   # parse\n",
    "\t\ttablesSe = soupSe.findAll('table')    # search for tables \n",
    "\t    # convert to a dataframe and label the data\n",
    "\t\tprint(i , j)   # this shows progress\n",
    "\t\tdfSe = pd.read_html(str(tablesSe[1]))[0]      # select the table of interest into a pandas dataframe\n",
    "\t\tdfSe.drop([1 , 2, 3] , inplace=True)\t      # only want the team data. (although might consider fouls against too)\n",
    "\t\tdfSe = dfSe.replace('Team' , powerSixTeams[j])   # want the name of the team \n",
    "\t\tnumRows = len(dfSe.index) # get the number of rows...... should just be one here\n",
    "\t\tdfSe['Team_ID'] = powerSixTeamID[j]        # put in the team ID's\n",
    "\t\tdfSe['Conf_ID'] = powerSixTeamsAndConf[j][1]        # put in the conference ID's  \n",
    "\t\tdfSe['Conf'] = powerSixConf[powerSixTeamsAndConf[j][1] - 1]       # put in the team's conference\n",
    "\t\tcurrYear = [i] * numRows  ## get the correct number \n",
    "\t\tdfSe['Date'] = currYear   # only want the year in this column\n",
    "\t\tseasonStats = seasonStats.append(dfSe , ignore_index=True) # add this to the season stats empty dataframe we started with \n",
    "seasonStats = seasonStats.rename(columns={'Unnamed: 0': 'Team'})  # simply clean up the columns \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets reorder the columns like we want to \n",
    "# moves the date to the first columm \n",
    "cols = seasonStats.columns.tolist()\n",
    "newCols = []\n",
    "newCols += [cols[-1]]  + [cols[0]] +  [cols[-4]] + [cols[-2]] +[cols[-3]] + cols[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seasonStats = seasonStats[newCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following few commands Provide visual representation of the data collected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLTeams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferenceTournamentGames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Organize Our Data into a useful dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's add the Win Loss percentages to the Season Stats DF\n",
    "# now we need to combine in a helpful way. Let's put the season stats next to each team in the games dataframe.\n",
    "team1DataDF = pd.DataFrame()    # placeHolder\n",
    "for i in range(len(seasonStats.index)):\n",
    "    year  = seasonStats.iloc[i,0]      # get year\n",
    "    team = seasonStats.iloc[i,1]       # get team\n",
    "    \n",
    "    yearIndex = WLTeams[WLTeams['year']==year].index.values.astype(int)\n",
    "    teamIndex = WLTeams[WLTeams['teams']==team].index.values.astype(int)  \n",
    "\n",
    "\n",
    "    ## intersection\n",
    "    \n",
    "    team1Index= set(yearIndex).intersection(teamIndex).pop()\n",
    "    placeHolder1 = pd.DataFrame([WLTeams.iloc[team1Index]])   # convert the desired data row into a dataframe\n",
    "    team1DataDF = team1DataDF.append(placeHolder1 ,  ignore_index=True)   \n",
    "seasonStats = pd.concat([seasonStats, team1DataDF], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonStats.drop(['teams' , 'year'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now we need to combine in a helpful way. Let's put the season stats next to each team in the games dataframe.\n",
    "final = pd.DataFrame()    # this is our final dataframe which we will output \n",
    "team1DataDF = pd.DataFrame()    # placeHolder\n",
    "team2DataDF = pd.DataFrame()    # placeHolder\n",
    "for i in range(len(conferenceTournamentGames.index)):\n",
    "    year  = conferenceTournamentGames.iloc[i,0]\n",
    "    team1 = conferenceTournamentGames.iloc[i,1]\n",
    "    team2 = conferenceTournamentGames.iloc[i,3]\n",
    "    yearIndex = seasonStats[seasonStats['Date']==year].index.values.astype(int)\n",
    "    firstIndex = seasonStats[seasonStats['Team']==team1].index.values.astype(int)  \n",
    "    secondIndex = seasonStats[seasonStats['Team']==team2].index.values.astype(int)  \n",
    "\n",
    "    ## intersect of first two for team 1\n",
    "    ## intersect of first and third for team2\n",
    "    \n",
    "    team1Index= set(yearIndex).intersection(firstIndex).pop()\n",
    "    team2Index= set(yearIndex).intersection(secondIndex).pop()\n",
    " \n",
    "    placeHolder1 = pd.DataFrame([seasonStats.iloc[team1Index]])   # convert the desired data row into a dataframe\n",
    "    placeHolder2 = pd.DataFrame([seasonStats.iloc[team2Index]])  # same for team two\n",
    "    team1DataDF = team1DataDF.append(placeHolder1 ,  ignore_index=True) \n",
    "    team2DataDF = team2DataDF.append(placeHolder2 ,  ignore_index=True)   \n",
    "\n",
    "# Add .1 to all the second team data, except the index labeled 'Date'\n",
    "names = ['Date']   # need to start with date in it. \n",
    "colNames = team2DataDF.columns\n",
    "for j in range(1 , len(colNames) , 1):\n",
    "    add = \".1\"\n",
    "    val = colNames[j] + add\n",
    "    names = names + [val]\n",
    "team2DataDF.columns = names   # this puts it back in as column indexes\n",
    "    \n",
    "# get the score of each tournament game\n",
    "third = pd.DataFrame(conferenceTournamentGames[conferenceTournamentGames.columns[[2,4]]])\n",
    "third.columns = ['Score' , 'Score.1']  \n",
    "# concatenate them together in the correct order\n",
    "final = pd.concat( ( team1DataDF  ,  team2DataDF , third ) ,  axis=1 , sort = False)   # we have one extra date column \n",
    "final = final.loc[:,~final.columns.duplicated()]  # this removes the duplicates. This is the reason we kept 'Date' as is earlier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual representation of the Final DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe shown below will be imported into the next python scrypt. Machhine learning will be used to generate odds for a any game based off of the teams season long statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the dataframes to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this sends our data to a csv file\n",
    "final.to_csv('PowerSixBball.csv', sep='\\t', encoding='utf-8')\n",
    "seasonStats.to_csv('SeasonStatistics.csv' , sep='\\t', encoding='utf-8')\n",
    "conferenceTournamentGames.to_csv('conference_tournament_games.csv' , sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
